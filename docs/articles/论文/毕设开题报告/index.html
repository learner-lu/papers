<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel='stylesheet' href=../../../css/prism.css /><link rel='stylesheet' href=../../../css/index.css />
    <link rel="icon" href="https://raw.githubusercontent.com/learner-lu/picbed/master/logo.png">

</head>
<body class="light">
    <div class='markdown-body'><h1>毕设开题报告</h1><h2>选题的目的和意义</h2><p>对于当前的服务器市场来说，存储和IO的带宽增长远远落后于算力的增长，主存的带宽增长远远无法跟上CPU核数的增长。对于现有的PCIe设备，其于主存的交互方式效率较低，比如GPU进行AI训练时需要将大量的数据分片，依次通过DMA从主存载入显存，网卡也需要通过DMA将数据装入主存，CPU再从主存中存取数据。其次PCIe设备内存相互割裂，存在PCIe设备之间无法直接交互，必须利用主存进行中转，多协处理器设备之间共享内存效率很低等问题</p><p>与此同时，随着人工智能、机器学习、分析、云基础设施、网络和边缘云化、通信系统和高性能计算等领域的持续发展，对数据的处理能力，以及处理多样性需求大大加剧。之前的每台服务器配备专用的内存，加速器以及网络设备的架构已经无法满足当前的现状，需要构建可扩展的内存池以满足日益增长的需求</p><p>基于以上情况，如今需要一种新型互联标准将不同模块之间相互链接，这种协议应该适用多种设备(FPGA， CPU， GPU， DPU等xPU)，可以满足与各种需求的计算(标量计算，矩阵计算，单指令多数据计算，多指令多数据计算)，可以协调各个计算资源保持数据并行有序计算，并且针对于不同的任务可以动态分配任务到相应的模块来处理。</p><p>Compute Express Link (CXL)[10]是一种Intel主导制定的一项CPU与其他设备开放互联标准，。CXL不仅支持IO设备互联，更加打通了设备的内存互联，缓存互联的开放互联标准，主要面向 CPU 和专用加速器的密集型工作负载。CXL在基于 PCI Express (PCIe) 的 I/O协议之上的增加了缓存的一致性和内存协议的支持，并提供处理器、内存扩展和加速器之间的低延迟高带宽互连，有力支撑AI、大数据等应用，对于越来越多计算需求无疑是非常重要的。</p><p>本课题"基于CXL的扩展内存池系统研究"旨在基于CXL的内存池系统结构，CXL。mem子协议，CXL驱动，并在gem5模拟器中构建多层次可扩展内存池，实现典型应用加速和能效提升，推动CXL在工业界的落地转化</p><h2>国内外研究现状和发展趋势</h2><p>截止到2022年，CXL共发布了三版标准，其中CXL1.1为CXL1.0的补充版</p><p>CXL1.1于2019年3月提出，支持CXL.io、CXL.cache 和 CXL.memory三种协议. CXL.io协议在功能上等同于 PCIe 协议，并利用PCIe的广泛行业采用和熟悉度作为其基础通信协议，适用于广泛的用例; CXL.cache协议专为更具体的应用程序而设计，使加速器能够有效地访问和缓存主机内存以优化性能; CXL.mem协议使主机（例如处理器）能够使用load/store命令访问设备连接的内存。这三个协议共同促进了计算设备之间内存资源的一致共享。</p><p>CXL2.0于2020年11月发布，CXL3.0与2022年8月发布，兼容CXL1.1和CXL1.0.CXL 3.0 引入了对直接内存访问和对内存池的增强，其中多个主机可以一致地共享 CXL 3.0 设备上的内存空间.CXL 2.0 支持单层交换，而CXL 3.0 引入了多层交换，支持交换结构的实施，借助 CXL 3.0，交换机可以连接到其他交换机，从而大大增加了扩展的可能性</p><p>可以通过CXL的三次大的发布看出CXL希望统一异构计算并且彻底改变服务器以后的架构。可以说CXL是intel对计算产业的一次巨大让利，基于CXL，实现了host与device之间的高效交互，缓解了CPU与IO之间的屏障，进而将host侧的算力需求逐步释放到device侧，更好地提升硬件的利用率，相信CXL会在后续硬件形态会扮演重要的角色。</p><p>由于CXL本身是一种开放的新型协议，可以很好的解决内存池扩展，异构计算，缓存一致性等问题。但由于目前还没有支持CXL内存扩展产品量产和商用，也还没有支持CXL的操作系统，目前学术界主要使用FPGA的方式实现CXL。公共云供应商通常寻求更高的性能要求和更低的硬件成本，在云计算的性能要求下，内存池的设计构建和扩展具有挑战性。虽然持久性内存（PM）和RDMA技术为实现高吞吐量和低延迟来存储和处理数据带来了新的机遇，但现有的分布式文件系统严格隔离了文件系统和网络层[9]，而且在新硬件建立一个高效的分布式存储系统是不容易的[8]。基于RDMA的的分解对于普通的工作负载来说涉及到太多的开销，而透明的延迟管理与虚拟化加速不兼容，因此针对基于CXL的分解系统和内存池控制器的研究[1]以及服务器集群中对于主内存的扩展和高效的内存管理[4]变得十分重要。而在异构计算方面，真正的异构系统应当使分区的工作负载可以被匹配到能获得最佳性能的硬件上，因此需要构建一个用于紧耦合异构计算的CXL性能模型[2]。由于现在深度学习(DL)训练耗费内存，每个计算组件也会受到内存容量和跨设备通信带宽的限制，模型参数通信正在成为分布式DL训练中的一个关键性能瓶颈，需要实现基于高速缓存一致性互联的内存扩展[3]。此外虽然通过RDMA进行的内存分解可以通过用远程内存访问来取代磁盘交换，可以提高内存受限的应用性能，但目前最先进的内存分解解决方案仍然使用为慢速磁盘设计的[7]，同时学界也在探索了基于CXL的直接访问的内存分解，通过CXL的内存协议（CXL.mem）直接连接了主机处理器群和远程内存资源[5]。</p><h2>研究内容，研究方法，技术路线及可行性分析</h2><p>鉴于CXL协议本身并没有具体规定厂商的实现细节，而且目前还没有稳定的产品或平台可以使用，因此本课题使用gem5模拟器搭建全系统模拟实现CXL</p><p>gem5[11]仿真基础设施是M5和GEMS仿真器的合并。M5提供了一个高度可配置的仿真框架，多种ISA，以及不同的CPU模型; GEMS用详细而灵活的内存系统补充了这些功能，包括支持多种高速缓存一致性协议和互连模型。该项目是许多学术机构和工业机构共同努力的结果，使得gem5成为一个有价值的全系统仿真工具。</p><p>gem5仿真基础设施允许研究人员在时钟周期水平上对现代计算机硬件进行建模，它有足够的保真度来启动未修改的基于Linux的操作系统，并运行包括x86、Arm和RISC-V在内的多种架构的完整应用程序，可以为计算机体系结构研究提供持续的改进和社区支持</p><p>本课题尝试在一个主机内实现CXL内存扩展，使用软件硬件模拟构建一个可以测试的平台系统，在其之上运行典型应用对比测试性能指标，针对CXL特性探究分级内存架构和内存管理的方案可行性。这个系统主要由CXL内存扩展设备和驱动程序组成，内存扩展设备用于扩展主机的内存容量，驱动程序负责检测设备的管理和使用</p><p>由于CXL扩展内存的实现原理和传统内存有所不同，传统的Linux内核内存管理方式是为和CPU直连的DRAM设计的，而CXL通过新的互联方式引入了容量更大，延迟更低的内存层级，所以需要为上层的应用提供一个统一的应用视图，使其兼容原先的操作系统内存管理方式。所以仍然需要软件层和驱动层的支持，例如三星针对其推出的CXL内存扩展器配套了对应的设备SMDK[6]，能够为现有的应用程序提供用户层的内存使用接口。</p><p>在内存池系统构建方面，本课题扩展gem5平台的bridge功能使其可以处理CXL内存池协议，构建新的CXL内存池设备以实现CXL数据存储，协议处理的功能，最后构建一致性的内存映射，使得CXL扩展设备内存池可以直接使用load/store指令随机访问</p><p>在典型应用下的系统评估方面，可以利用gem5的性能统计模块，判断内存池的整体消耗以及应用的执行时间，平均读写访问延迟，以此来统计分析大数据应用在不同工作负载下的内存访问特征，以验证和评估CXL的功能特性</p><p>在驱动程序方面，主要是基于Linux内核驱动模块的代码编写，驱动应当可以做到设备的识别和映射，并提供mmap接口将内存映射到内存的虚拟地址空间中，同时应能在内核中记录设备的内存使用情况。</p><h2>项目特色和创新点</h2><p>CXL很重要的特点是面向解决内存池扩展，异构计算，缓存一致性等问题，主要分为三种应用场景，第一类设备例如智能网卡，DPU等运算加速设备;第二类设备例如GPU，可以做异构计算;第三类设备是PCIe设备用于做内存扩展.目前工业界最有可能使CXL应用落地是第三类内存扩展设备作为内存池扩展.其次由于目前异构计算方面还没有可商用的设备和平台，模拟实现难度过大，因此本课题选择去尝试内存池方面的研究.</p><p>本课题的创新点在于使用gem5模拟器搭建一个软件层面的可用CXL系统，尝试探索CXL特性，构建和优化CXL可扩展内存池，设计CXL内存管理方案为已有应用提供一个统一的内存使用接口，编写驱动程序以适配PCIe设备的识别和通信.</p><h2>进度安排</h2><p>课题时间： 2022 年 12 月 至   2023  年  5  月</p><ul><li>文献阅读和资料查询     2022.12 – 2023.1</li></ul><ul><li>实验设计和模拟环境搭建 2023.1  - 2023.2</li></ul><ul><li>实验代码编写和模拟测试 2023.2  - 2023.4</li></ul><ul><li>论文报告撰写           2023.4  - 2023.5</li></ul><ul><li>论文答辩               2023.5  - 2023.6</li></ul><h2>主要参考文献</h2><ul><li>[1] Li H, Berger D S, Novakovic S, et al. First-generation Memory Disaggregation for Cloud Platforms[J]. arXiv preprint arXiv:2203.00241， 2022.</li></ul><ul><li>[2] Cabrera A M, Young A R, Vetter J S. Design and analysis of CXL performance models for tightly-coupled heterogeneous computing[C]//Proceedings of the 1st International Workshop on Extreme Heterogeneity Solutions. 2022: 1-6.</li></ul><ul><li>[3] Wang Z, Sim J, Lim E, et al. Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems[C]//2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022: 126-140.</li></ul><ul><li>[4] Maruf H A, Wang H, Dhanotia A, et al. TPP: Transparent Page Placement for CXL-Enabled Tiered Memory[J]. arXiv preprint arXiv:2206.02878, 2022.</li></ul><ul><li>[5] Gouk D, Lee S, Kwon M, et al. Direct Access,{High-Performance} Memory Disaggregation with {DirectCXL}[C]//2022 USENIX Annual Technical Conference (USENIX ATC 22). 2022: 287-294.</li></ul><ul><li>[6] SMDK:Scalable Memory Development Kit.https://github.com/OpenMPDK/SMDK/wiki</li></ul><ul><li>[7] Al Maruf H, Chowdhury M. Effectively prefetching remote memory with leap[C]//2020 USENIX Annual Technical Conference (USENIX ATC 20). 2020: 843-857.</li></ul><ul><li>[8] Shu J, Chen Y, Wang Q, et al. Th-dpms: Design and implementation of an rdma-enabled distributed persistent memory storage system[J]. ACM Transactions on Storage (TOS), 2020, 16(4): 1-31.</li></ul><ul><li>[9] Lu Y, Shu J, Chen Y, et al. Octopus: an {RDMA-enabled} Distributed Persistent Memory File System[C]//2017 USENIX Annual Technical Conference (USENIX ATC 17). 2017: 773-785.</li></ul><ul><li>[10] Compute Express Link (CXL). https://www.computeexpresslink.org/.</li></ul><ul><li>[11]  Binkert N, Beckmann B, Black G, et al. The gem5 simulator[J]. ACM SIGARCH computer architecture news, 2011, 39(2): 1-7.</li></ul></div>
    <div class="dir-tree"><ul><li><a href="../../md-docs/README" >README</a></li></ul><ul><li><a href="../../论文/CXL-SSD" >论文<ul><li><a href="../../论文/CXL-SSD" >CXL-SSD</a></li></ul><ul><li><a href="../../论文/毕设开题报告" >毕设开题报告</a></li></ul></a></li></ul></div>
    <p><a class="zood" href="https://github.com/luzhixing12345/zood" target="_blank" >zood</a></p>
    <script type="text/javascript" src="../../../js/change_mode.js"></script><script>addChangeModeButton("../../../img/sun.png","../../../img/moon.png")</script><script type="text/javascript" src="../../../js/copy_code.js"></script><script>addCodeCopy("../../../img/before_copy.png","../../../img/after_copy.png")</script><script type="text/javascript" src="../../../js/prism.js"></script><script type="text/javascript" src="../../../js/next_front.js"></script><script>addLink("../../论文/CXL-SSD",".","ab")</script><script type="text/javascript" src="../../../js/picture_preview.js"></script><script type="text/javascript" src="../../../js/check_box.js"></script>
</body>
</html>